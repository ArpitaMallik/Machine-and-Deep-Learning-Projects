{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95426,"databundleVersionId":11344607,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:32:03.917157Z","iopub.execute_input":"2025-04-08T06:32:03.917481Z","iopub.status.idle":"2025-04-08T06:32:05.056703Z","shell.execute_reply.started":"2025-04-08T06:32:03.917452Z","shell.execute_reply":"2025-04-08T06:32:05.055546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/aiquest-bangla-sentiment-analysis-competition/train.csv\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:32:10.659286Z","iopub.execute_input":"2025-04-08T06:32:10.659710Z","iopub.status.idle":"2025-04-08T06:32:10.711972Z","shell.execute_reply.started":"2025-04-08T06:32:10.659677Z","shell.execute_reply":"2025-04-08T06:32:10.710742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:24:56.067499Z","iopub.execute_input":"2025-03-31T15:24:56.067829Z","iopub.status.idle":"2025-03-31T15:24:56.072900Z","shell.execute_reply.started":"2025-03-31T15:24:56.067797Z","shell.execute_reply":"2025-03-31T15:24:56.072255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text'].isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:24:56.074161Z","iopub.execute_input":"2025-03-31T15:24:56.074412Z","iopub.status.idle":"2025-03-31T15:24:56.090744Z","shell.execute_reply.started":"2025-03-31T15:24:56.074386Z","shell.execute_reply":"2025-03-31T15:24:56.089953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['sentiment'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:24:56.091826Z","iopub.execute_input":"2025-03-31T15:24:56.092175Z","iopub.status.idle":"2025-03-31T15:24:56.113471Z","shell.execute_reply.started":"2025-03-31T15:24:56.092150Z","shell.execute_reply":"2025-03-31T15:24:56.112392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef text_preprocessing(text):\n    text = re.sub(r'[@#\\/]\\S+', '', text)\n\n    text = re.sub(r'\\d+', '', text)\n\n    text = re.sub(r'\\d+', '', text)  # English numbers\n    text = re.sub(r'[০-৯]+', '', text)  # Bangla numbers\n\n    text = re.sub(r'https?:\\/\\/\\S+|www\\.\\S+|ftp:\\/\\/\\S+|mailto:\\S+|https?:', '', text)\n\n    text = text.replace('\\n', ' ').replace('\\r', '')\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    english_punc = string.punctuation\n\n    bangla_punc = '।,?!:;\"\\'()-—॥‘’“”'  \n\n    all_punc = english_punc + bangla_punc\n    text = text.translate(str.maketrans('', '', all_punc))\n\n    emoji_pattern = re.compile(\n        \"[\"\n        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n        \"\\U00002700-\\U000027BF\"  # Dingbats\n        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n        \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n        \"\\U00002B50-\\U00002B55\"  # Stars and other symbols\n        \"]+\",\n        flags=re.UNICODE\n    )\n    text = emoji_pattern.sub(r'', text)\n    \n    # Bangla stopwords\n    stop_words = {'এ', 'হয়', 'কি', 'কী', 'এর', 'কে', 'যে', 'এই', 'বা', 'সব', 'টি', 'তা',\n                  'সে', 'তাই', 'সেই', 'তার', 'আগে', 'যদি', 'আছে', 'আমি', 'এবং', 'করে', 'কার', 'এটি', 'হতে', 'যায়',\n                  'আরও', 'যাক', 'খুব', 'উপর', 'পরে', 'হবে', 'কেন', 'কখন', 'সকল', 'হয়', 'ঠিক', 'একই', 'কোন',\n                  'ছিল', 'খুবই', 'কোনো', 'অধীন', 'যারা', 'তারা', 'গুলি', 'তাকে', 'সেটা', 'সময়', 'আমার', 'আমরা', 'সবার',\n                  'উভয়', 'একটা', 'আপনি', 'নিয়ে', 'একটি', 'বন্ধ', 'জন্য','জন্য', 'শুধু', 'যেটা', 'উচিত', 'মাঝে', 'থেকে', 'করবে',\n                  'আবার', 'উপরে', 'সেটি', 'কিছু', 'কারণ', 'যেমন', 'তিনি', 'মধ্যে', 'আমাকে', 'করছেন', 'তুলনা', 'তারপর',\n                  'নিজেই', 'থাকার', 'নিজের', 'পারেন', 'একবার', 'সঙ্গে', 'ইচ্ছা', 'নীচের', 'এগুলো', 'আপনার', 'অধীনে', 'কিংবা',\n                  'এখানে', 'তাহলে', 'কয়েক', 'জন্যে', 'হচ্ছে', 'তাদের', 'কোথায়', 'কিন্তু', 'নিজেকে', 'যতক্ষণ', 'আমাদের',\n                  'দ্বারা', 'হয়েছে', 'সঙ্গে', 'সেখানে', 'কিভাবে', 'মাধ্যমে', 'নিজেদের', 'তুলনায়', 'প্রতিটি',\n                  'তাদেরকে', 'ইত্যাদি', 'সম্পর্কে', 'সর্বাধিক', 'বিরুদ্ধে', 'অন্যান্য','প্রায়ই'}\n\n\n    text_ls = text.split()\n    filtered_words = [word for word in text_ls if word not in stop_words]\n   \n    text = \" \".join(filtered_words)\n\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:32:16.574020Z","iopub.execute_input":"2025-04-08T06:32:16.574382Z","iopub.status.idle":"2025-04-08T06:32:16.585529Z","shell.execute_reply.started":"2025-04-08T06:32:16.574321Z","shell.execute_reply":"2025-04-08T06:32:16.583994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text'] = train['text'].apply(text_preprocessing)\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:32:21.547057Z","iopub.execute_input":"2025-04-08T06:32:21.547428Z","iopub.status.idle":"2025-04-08T06:32:21.567405Z","shell.execute_reply.started":"2025-04-08T06:32:21.547400Z","shell.execute_reply":"2025-04-08T06:32:21.566277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Convert sentiment labels to numeric codes\nlabel_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\nreverse_label_mapping = {v: k for k, v in label_mapping.items()}\n\ntrain['sentiment_encoded'] = train['sentiment'].map(label_mapping)\n\n# Define Features (X) and Target (y)\nX = train['text']\ny = train['sentiment_encoded']\n\n# TF-IDF Vectorization\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)\nX_tfidf = vectorizer.fit_transform(X)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n\n# Define models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=500),\n    \"Naïve Bayes\": MultinomialNB(),\n    \"SVM\": SVC(kernel='linear'),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500)\n}\n\n# Dictionary to store F1 scores\nmodel_f1_scores = {}\nbest_model = None\nbest_f1 = 0\n\n# Train and evaluate each model\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    f1 = f1_score(y_test, y_pred, average='macro')\n    model_f1_scores[name] = f1\n    \n    print(f\"{name} Macro F1 Score: {f1:.4f}\")\n    print(f\"{name} Classification Report:\\n\")\n    print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive']))\n    print(\"=\" * 50)\n\n    # Track best model\n    if f1 > best_f1:\n        best_f1 = f1\n        best_model = model\n        best_model_name = name\n\nprint(f\"\\nBest Model: {best_model_name} with Macro F1 Score: {best_f1:.4f}\")\n\n# Retrain best model on full training data\nbest_model.fit(X_tfidf, y)\n\n# Predict on full training data\ny_pred_train = best_model.predict(X_tfidf)\npredicted_labels = [reverse_label_mapping[p] for p in y_pred_train]\n\n# Create submission DataFrame\nsubmission = pd.DataFrame({\n    \"id\": range(len(train)),\n    \"sentiment\": predicted_labels\n})\n\n# Save to CSV\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"\\nPredictions saved in `submission.csv` using the best model based on Macro F1 Score!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:38:04.435647Z","iopub.execute_input":"2025-04-08T06:38:04.436017Z","iopub.status.idle":"2025-04-08T06:38:05.691688Z","shell.execute_reply.started":"2025-04-08T06:38:04.435991Z","shell.execute_reply":"2025-04-08T06:38:05.690548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import numpy as np\n# from sklearn.metrics import classification_report, f1_score, confusion_matrix\n# from transformers import (\n#     BertForSequenceClassification,\n#     AdamW,\n#     AutoTokenizer,\n#     get_linear_schedule_with_warmup\n# )\n# from torch.utils.data import Dataset, DataLoader\n# from tqdm import tqdm\n# import pandas as pd\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.model_selection import train_test_split\n\n# # Set device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Hyperparameters\n# LEARNING_RATE = 2e-5\n# BATCH_SIZE = 32  # Updated to 32\n# NUM_EPOCHS = 10\n# PATIENCE = 3  # Early stopping patience\n# LABELS = [0, 1, 2]  # Changed to numeric labels\n# MODEL_NAME = \"sagorsarker/bangla-bert-base\"\n\n# label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n# train['sentiment_encoded'] = train['sentiment'].map(label_mapping)\n\n# train_df, val_df = train_test_split(train, test_size=0.2, random_state=42, stratify=train[\"sentiment_encoded\"])\n\n\n# # Initialize tokenizer and model\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n# model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABELS)).to(device)\n\n# # Optimizer and Scheduler\n# optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n# num_train_steps = len(train_df) // BATCH_SIZE * NUM_EPOCHS\n# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n\n# # Dataset Class\n# class BanglaDataset(Dataset):\n#     def __init__(self, dataframe, tokenizer, max_length=128):\n#         self.dataframe = dataframe\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __len__(self):\n#         return len(self.dataframe)\n\n#     def __getitem__(self, idx):\n#         text = self.dataframe.iloc[idx][\"text\"]\n#         label = self.dataframe.iloc[idx][\"sentiment_encoded\"]  # Updated label usage\n\n#         encoding = self.tokenizer(\n#             text,\n#             max_length=self.max_length,\n#             padding=\"max_length\",\n#             truncation=True,\n#             return_tensors=\"pt\"\n#         )\n\n#         return {\n#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n#             \"label\": torch.tensor(label, dtype=torch.long)\n#         }\n\n# # Prepare DataLoaders\n# train_dataset = BanglaDataset(train_df, tokenizer)\n# val_dataset = BanglaDataset(val_df, tokenizer)\n\n# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# # Function to evaluate model\n# def evaluate(model, val_loader):\n#     model.eval()\n#     total_loss = 0\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for batch in val_loader:\n#             input_ids = batch[\"input_ids\"].to(device)\n#             attention_mask = batch[\"attention_mask\"].to(device)\n#             labels = batch[\"label\"].to(device)\n\n#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#             loss = outputs.loss\n#             logits = outputs.logits\n\n#             total_loss += loss.item()\n#             preds = torch.argmax(logits, dim=1)\n#             correct += (preds == labels).sum().item()\n#             total += labels.size(0)\n\n#     avg_loss = total_loss / len(val_loader)\n#     accuracy = correct / total\n#     return avg_loss, accuracy\n\n# # Training Loop with Early Stopping\n# best_val_loss = float(\"inf\")\n# early_stop_counter = 0\n# best_model_path = \"best_bangla_bert.pth\"\n\n# for epoch in range(NUM_EPOCHS):\n#     model.train()\n#     total_train_loss = 0\n\n#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\"):\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         labels = batch[\"label\"].to(device)\n\n#         model.zero_grad()\n#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#         loss = outputs.loss\n#         total_train_loss += loss.item()\n\n#         loss.backward()\n#         optimizer.step()\n#         scheduler.step()\n\n#     avg_train_loss = total_train_loss / len(train_loader)\n\n#     # Evaluate on validation set\n#     val_loss, val_accuracy = evaluate(model, val_loader)\n\n#     print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n\n#     # Save model if validation loss improves\n#     if val_loss < best_val_loss:\n#         best_val_loss = val_loss\n#         early_stop_counter = 0\n#         torch.save(model.state_dict(), best_model_path)\n#         print(f\"New best model saved at epoch {epoch+1}\")\n#     else:\n#         early_stop_counter += 1\n#         print(f\"Validation loss increased ({early_stop_counter}/{PATIENCE})\")\n\n#     # Stop training if validation loss keeps degrading\n#     if early_stop_counter >= PATIENCE:\n#         print(\"Early stopping triggered. Training stopped.\")\n#         break\n\n# # Load the best model before inference\n# model.load_state_dict(torch.load(best_model_path))\n# print(\"\\nBest model loaded for inference.\")\n\n# # Function to get predictions\n# def get_predictions(model, data_loader):\n#     model.eval()\n#     predictions, true_labels = [], []\n\n#     with torch.no_grad():\n#         for batch in data_loader:\n#             input_ids = batch[\"input_ids\"].to(device)\n#             attention_mask = batch[\"attention_mask\"].to(device)\n#             labels = batch[\"label\"].to(device)\n\n#             outputs = model(input_ids, attention_mask=attention_mask)\n#             logits = outputs.logits\n#             preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n#             predictions.extend(preds)\n#             true_labels.extend(labels.cpu().numpy())\n\n#     return np.array(true_labels), np.array(predictions)\n\n# # Get True Labels & Predictions\n# y_true, y_pred = get_predictions(model, val_loader)\n\n# # Classification Report\n# print(\"Classification Report:\")\n# print(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n\n# # Macro F1 Score\n# macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n# print(f\"\\nMacro F1 Score: {macro_f1:.4f}\")\n\n# # Confusion Matrix\n# conf_matrix = confusion_matrix(y_true, y_pred)\n# print(\"\\nConfusion Matrix:\")\n# print(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:24:56.257432Z","iopub.execute_input":"2025-03-31T15:24:56.257746Z","iopub.status.idle":"2025-03-31T15:24:56.270158Z","shell.execute_reply.started":"2025-03-31T15:24:56.257715Z","shell.execute_reply":"2025-03-31T15:24:56.269341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import classification_report, f1_score, confusion_matrix\n# import torch\n# import numpy as np\n# import pandas as pd\n\n# # Function to get predictions on a given dataset\n# def get_predictions(model, data_loader):\n#     model.eval()\n#     predictions, true_labels = [], []\n\n#     with torch.no_grad():\n#         for batch in data_loader:\n#             input_ids = batch[\"input_ids\"].to(device)\n#             attention_mask = batch[\"attention_mask\"].to(device)\n#             labels = batch[\"label\"].to(device)  # True labels\n\n#             outputs = model(input_ids, attention_mask=attention_mask)\n#             logits = outputs.logits\n#             preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n#             predictions.extend(preds)\n#             true_labels.extend(labels.cpu().numpy())\n\n#     return np.array(true_labels), np.array(predictions)\n\n# y_true_train, y_pred_train = get_predictions(model, train_loader)\n\n# print(\"Classification Report (Train Data):\")\n# print(classification_report(y_true_train, y_pred_train, target_names=[\"negative\", \"neutral\", \"positive\"]))\n\n# macro_f1_train = f1_score(y_true_train, y_pred_train, average=\"macro\")\n# print(f\"\\nMacro F1 Score (Train Data): {macro_f1_train:.4f}\")\n\n# conf_matrix_train = confusion_matrix(y_true_train, y_pred_train)\n# print(\"\\nConfusion Matrix (Train Data):\")\n# print(conf_matrix_train)\n\n# label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n# predicted_labels_train = [label_mapping[p] for p in y_pred_train]\n\n\n# submission = pd.DataFrame({\n#     \"id\": range(len(predicted_labels_train)),  # Sequential IDs\n#     \"sentiment\": predicted_labels_train\n# })\n\n# # Save to CSV\n# submission.to_csv(\"submission.csv\", index=False)\n# print(\"\\nPredictions saved in `submission.csv`!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:24:56.271842Z","iopub.execute_input":"2025-03-31T15:24:56.272143Z","iopub.status.idle":"2025-03-31T15:24:56.287647Z","shell.execute_reply.started":"2025-03-31T15:24:56.272121Z","shell.execute_reply":"2025-03-31T15:24:56.286509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import numpy as np\n# import pandas as pd\n# from transformers import (\n#     BertForSequenceClassification,\n#     AdamW,\n#     AutoTokenizer,\n#     get_linear_schedule_with_warmup\n# )\n# from torch.utils.data import Dataset, DataLoader\n# from tqdm import tqdm\n\n# # Set device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n# train['sentiment_encoded'] = train['sentiment'].map(label_mapping)\n\n\n# # Hyperparameters\n# LEARNING_RATE = 1e-5\n# BATCH_SIZE = 8\n# NUM_EPOCHS = 15\n# MODEL_NAME = \"sagorsarker/bangla-bert-base\"\n\n# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# MODEL_NAME = \"csebuetnlp/banglabert_large\"\n\n# # Load tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# # Load pre-trained model for classification\n# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)  # Assuming 3 sentiment classes\n# model.to(device)  # Move to GPU if available\n\n\n# # Dataset Class\n# class BanglaDataset(Dataset):\n#     def __init__(self, dataframe, tokenizer, max_length=128):\n#         self.dataframe = dataframe\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __len__(self):\n#         return len(self.dataframe)\n\n#     def __getitem__(self, idx):\n#         text = self.dataframe.iloc[idx][\"text\"]\n#         label = self.dataframe.iloc[idx][\"sentiment_encoded\"]\n\n#         encoding = self.tokenizer(\n#             text,\n#             max_length=self.max_length,\n#             padding=\"max_length\",\n#             truncation=True,\n#             return_tensors=\"pt\"\n#         )\n\n#         return {\n#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n#             \"label\": torch.tensor(label, dtype=torch.long)\n#         }\n\n# # Load full training dataset\n# train_dataset = BanglaDataset(train, tokenizer)\n# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# # Optimizer and Scheduler\n# optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=0.01)\n# num_train_steps = len(train_loader) * NUM_EPOCHS\n# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n\n# # Training Loop\n# for epoch in range(NUM_EPOCHS):\n#     model.train()\n#     total_train_loss = 0\n\n#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\"):\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         labels = batch[\"label\"].to(device)\n\n#         model.zero_grad()\n#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#         loss = outputs.loss\n#         total_train_loss += loss.item()\n\n#         loss.backward()\n#         optimizer.step()\n#         scheduler.step()\n\n#     print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {total_train_loss / len(train_loader):.4f}\")\n\n# # Save trained model\n# torch.save(model.state_dict(), \"final_bangla_bert.pth\")\n# print(\"\\nModel training complete. Model saved.\")\n\n# # Prediction Function\n# def get_predictions(model, dataset):\n#     model.eval()\n#     predictions = []\n\n#     with torch.no_grad():\n#         for i in range(len(dataset)):\n#             sample = dataset[i]\n#             input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n#             attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n\n#             logits = model(input_ids, attention_mask=attention_mask).logits\n#             pred = torch.argmax(logits, dim=1).item()\n#             predictions.append(pred)\n\n#     return np.array(predictions)\n\n# # Predict on training data\n# y_pred_train = get_predictions(model, train_dataset)\n\n# # Convert numeric predictions to labels\n# label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n# predicted_labels = [label_mapping[p] for p in y_pred_train]\n\n# # Create submission DataFrame\n# submission = pd.DataFrame({\n#     \"id\": range(len(train)),  # Sequential IDs\n#     \"sentiment\": predicted_labels\n# })\n\n# # Save to CSV\n# submission.to_csv(\"submission.csv\", index=False)\n# print(\"\\nPredictions saved in `submission.csv`!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:47:23.611318Z","iopub.execute_input":"2025-03-31T15:47:23.611754Z","iopub.status.idle":"2025-03-31T15:50:06.160427Z","shell.execute_reply.started":"2025-03-31T15:47:23.611711Z","shell.execute_reply":"2025-03-31T15:50:06.159472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/working/submission.csv\")\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T06:38:30.332949Z","iopub.execute_input":"2025-04-08T06:38:30.333321Z","iopub.status.idle":"2025-04-08T06:38:30.345529Z","shell.execute_reply.started":"2025-04-08T06:38:30.333293Z","shell.execute_reply":"2025-04-08T06:38:30.344218Z"}},"outputs":[],"execution_count":null}]}